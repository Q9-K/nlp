# Subword 切分方法对比实验报告
> 蒋康
> 202518014628074

## 实验方法

### 1. BPE (Byte Pair Encoding)

BPE 是一种基于频率的子词切分算法。其基本思想是：
1. 初始化词表为所有字符
2. 统计所有相邻字符对的出现频率
3. 将出现频率最高的字符对合并为新的子词
4. 重复步骤 2-3 直到达到目标词表大小

**实现**: 使用 SentencePiece 库，设置 `model_type='bpe'`

### 2. WordPiece

WordPiece 与 BPE 类似，但选择合并的标准不同：
1. 初始化词表为所有字符
2. 计算合并每对字符后对训练数据似然值的提升
3. 选择使似然值提升最大的字符对进行合并
4. 重复直到达到目标词表大小

**实现**: 使用 Hugging Face tokenizers 库

### 3. Unigram

Unigram 是一种基于概率的子词模型：
1. 从一个较大的初始词表开始
2. 使用 EM 算法估计每个子词的概率
3. 计算移除每个子词对损失函数的影响
4. 移除影响最小的子词，直到达到目标词表大小

**实现**: 使用 SentencePiece 库，设置 `model_type='unigram'`

## 评价指标

1. **Token 数**: 切分后的 token 总数
2. **字符压缩率**: 原始字符数 / Token 数，表示每个 token 平均代表的字符数
3. **词/Token 比**: 原始词数 / Token 数，越接近 1 表示切分粒度越接近词级别

## 实验结果

### 词表规模 1000

| 方法 | Token数 | 字符压缩率 | 词/Token比 |
|------|---------|------------|------------|
| BPE | 8,499,281 | 2.80 | 0.4469 |
| WordPiece | 8,741,356 | 2.72 | 0.4346 |
| Unigram | 8,417,309 | **2.83** | **0.4513** |

### 词表规模 3000

| 方法 | Token数 | 字符压缩率 | 词/Token比 |
|------|---------|------------|------------|
| BPE | 6,328,558 | 3.76 | 0.6002 |
| WordPiece | 6,323,855 | 3.76 | 0.6007 |
| Unigram | 6,198,352 | **3.84** | **0.6128** |

### 词表规模 5000

| 方法 | Token数 | 字符压缩率 | 词/Token比 |
|------|---------|------------|------------|
| BPE | 5,666,595 | 4.20 | 0.6703 |
| WordPiece | 5,633,979 | 4.22 | 0.6742 |
| Unigram | 5,574,317 | **4.27** | **0.6814** |

### 综合对比表

| 方法 | 词表规模 | Token数 | 字符压缩率 | 词/Token比 |
|------|----------|---------|------------|------------|
| BPE | 1000 | 8,499,281 | 2.80 | 0.4469 |
| WordPiece | 1000 | 8,741,356 | 2.72 | 0.4346 |
| Unigram | 1000 | 8,417,309 | 2.83 | 0.4513 |
| BPE | 3000 | 6,328,558 | 3.76 | 0.6002 |
| WordPiece | 3000 | 6,323,855 | 3.76 | 0.6007 |
| Unigram | 3000 | 6,198,352 | 3.84 | 0.6128 |
| BPE | 5000 | 5,666,595 | 4.20 | 0.6703 |
| WordPiece | 5000 | 5,633,979 | 4.22 | 0.6742 |
| Unigram | 5000 | 5,574,317 | 4.27 | 0.6814 |

## 结果分析

### 1. 不同方法的压缩效果对比

- **Unigram** 在所有词表规模下都取得了最高的压缩率，这是因为 Unigram 模型通过概率优化直接最大化了数据的似然值
- **BPE** 和 **WordPiece** 的效果相近，在较大词表（5000）时 WordPiece 略优于 BPE
- 在小词表（1000）情况下，三种方法的差异更为明显

### 2. 词表规模对压缩率的影响

| 词表规模 | 平均压缩率 | 压缩率提升 |
|----------|------------|------------|
| 1000 | 2.78 | - |
| 3000 | 3.79 | +36% |
| 5000 | 4.23 | +12% |

- 词表规模从 1000 增加到 3000 时，压缩率提升显著（约 36%）
- 从 3000 增加到 5000 时，提升趋于平缓（约 12%）
- 这说明存在边际效应递减，并非词表越大越好